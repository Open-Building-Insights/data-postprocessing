{"cells":[{"cell_type":"markdown","metadata":{},"source":["## building_matching\n","### Matches respective buildings between two different building footprint catalog loosely based on the so called Hungarian Method"]},{"cell_type":"markdown","metadata":{},"source":["### Initial configuration\n","#### To start working with this particular notebook, you need to provide necessary credential and settings\n","#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n","    \"\"\"\n","    {\n","    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n","    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n","    \"COS_APIKEY\": \"xxx\",\n","    \"DB2_CONNECTION_STRING\": \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\",\n","    \"DB2_USERNAME\": \"xxx\",\n","    \"DB2_PASSWORD\": \"xxx\",\n","    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n","    \"ENERGY_ESTIMATION_BUCKET\": \"kenya-energy-estimation-matching\",\n","    \"JOB_STATUS_BUCKET\": \"notebook-job-status\",\n","    \"COUNTRY_TABLE\": \"FEATURES_DB_MAHARASHTRA\",\n","    \"AREA_THRESHOLD\": 20\n","    }\n","    \"\"\"\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Read notebook configuration\n","import getpass\n","import json\n","\n","config_str = getpass.getpass('Enter your prepared config: ')\n","config = json.loads(config_str)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# # ! pip install pykml\n","# # ! pip install mgrs\n","# ! pip install shapely\n","\n","# ! pip install ibmcloudant\n","# ! pip install geopandas\n","# ! pip install rasterio==1.3.8\n","# ! pip install numpy==1.23.5; python_version >= '3.7'\n","# ! pip install pyproj==3.6.0\n","# ! pip install pathos==0.3.1\n","# ! pip install ibmcloudant==0.4.3\n","# ! pip install ibm-cloud-sdk-core==3.16.7"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import json\n","import pandas as pd\n","import geopandas as gpd\n","import numpy as np\n","import shapely\n","from tqdm import tqdm\n","from pyproj import Geod\n","from collections import Counter\n","import os\n","from botocore.client import Config\n","from pyproj import Geod\n","import ibm_boto3\n","from scipy import optimize as sci_opt\n","import requests\n","import warnings\n","import boto3\n","from botocore import UNSIGNED\n","# import botocore.response as br\n","import gzip\n","import jaydebeapi as jdbc\n","import jpype\n","import zipfile\n","from io import BytesIO\n","import io\n","import shutil\n","import shapely\n","\n","warnings.simplefilter(\"ignore\", category=FutureWarning)\n","\n","geod = Geod(ellps=\"WGS84\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# S3client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n","\n","\n","cos_client = ibm_boto3.client(service_name='s3',\n","                              ibm_api_key_id=config[\"COS_APIKEY\"],\n","                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n","                              config=Config(signature_version='oauth'),\n","                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["External utils succesfully imported\n"]}],"source":["response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n","\n","# download utils module\n","try:\n","    from utils import *\n","    print('External utils succesfully imported')\n","    \n","except Exception as e:\n","    print('Desired packages is missing in local env, downloading it...', e)\n","    for obj in response['Contents']:\n","        name = obj['Key']\n","        streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n","        print(\"Downloading to localStorage :  \" + name)\n","        with io.FileIO(name, 'w') as file:\n","            for i in io.BytesIO(streaming_body_1.read()):\n","                file.write(i)\n","    from utils import *\n","    print('External utils succesfully imported')\n","  "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["base_path = os.getcwd()\n","jsons_path = os.path.join(base_path, 'tiles_jsons/')\n","geojsons_path = 'kenya_energy_estimated_geojsons'\n","sql_tablename = config[\"COUNTRY_TABLE\"]\n","batch_limit = '370_000<'\n","trm = 'https://biospheric-vector.s3.amazonaws.com/'\n","area_threshold = config[\"AREA_THRESHOLD\"]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["matched_buildings = cos_client.list_objects_v2(Bucket=config[\"ENERGY_ESTIMATION_BUCKET\"])\n","processed_ids = [int(i['Key'].split('_')[0]) for i in matched_buildings['Contents']]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# connect to the IBM DB2 function\n","def connect_to_db():\n","\n","    jar = 'db2jcc4.jar'\n","    os.environ['CLASSPATH'] = jar\n","\n","    args='-Djava.class.path=%s' % jar\n","    jvm_path = jpype.getDefaultJVMPath()\n","    try:\n","        jpype.startJVM(jvm_path, args)\n","    except Exception as e:\n","        print('startJVM exception: ', e)\n","        \n","    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n","        jpype.attachThreadToJVM()\n","        jpype.java.lang.Thread.currentThread().setContextClassLoader(jpype.java.lang.ClassLoader.getSystemClassLoader())\n","        \n","    # create JDBC connection\n","    conn = jdbc.connect(\n","                'com.ibm.db2.jcc.DB2Driver',\n","                config['DB2_CONNECTION_STRING'],\n","                [config[\"DB2_USERNAME\"], config[\"DB2_PASSWORD\"]],\n","                'db2jcc4.jar')\n","    \n","    return conn\n","\n","def fetch_builings_in_bbox(cursor, lon_min, lon_max, lat_min, lat_max):\n","    '''\n","        This particular function is aimed for obtating all entries from defined rectangle for selected SQL table\n","    '''\n","\n","    # fetch column names from defined SQL table\n","\n","    columns = ['latitude', 'longitude', 'polygon_coordinates', 'vida_confidence']\n","    \n","    # sql statement for selecting entries by defined rectangle boundaries\n","    sql = f\"\"\"\n","        SELECT {', '.join(columns)} FROM USER1.{sql_tablename}\n","        WHERE \n","            (LATITUDE >= {lat_min}) AND \n","            (LATITUDE <= {lat_max}) AND \n","            (LONGITUDE >= {lon_min}) AND \n","            (LONGITUDE <= {lon_max}) AND\n","            (AREA_IN_METERS > {area_threshold}) AND\n","            (FOOTPRINT_SOURCE != 'osm')\n","        \"\"\"\n","    \n","    try:\n","        cursor.execute(sql)\n","        data = cursor.fetchall()\n","    except Exception as e:\n","        print(f\"Fetch items error occured: {e}\")\n","        print(\"Reconnecting to the database try again...\")\n","\n","        conn = connect_to_db()\n","        cursor = conn.cursor()\n","        cursor.execute(sql)\n","        data = cursor.fetchall()\n","        \n","    finally:\n","        # reshape obtained data to the GeoDataFrame\n","        df = pd.DataFrame(data=data, columns=columns)\n","        df = gpd.GeoDataFrame(df, geometry=shapely.from_wkt(df.polygon_coordinates.astype(str)))\n","        df = df.drop(['polygon_coordinates'], axis=1)\n","        \n","        df.geometry = shapely.set_precision(df.geometry.array, grid_size=0.000001)\n","        \n","        df['area_in_meters'] = df[\"geometry\"].apply(lambda g: abs(geod.geometry_area_perimeter(g)[0]))\n","\n","        return df\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["dlinks_df = gpd.read_parquet('buildings_energy_estimates_links.parquet')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid_cell_id</th>\n","      <th>download_link</th>\n","      <th>geometry</th>\n","      <th>buildings_amount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>525390</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((36.50000 -1.00000, 36.75000 -1.00000...</td>\n","      <td>389994.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>525391</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((36.75000 -1.00000, 37.00000 -1.00000...</td>\n","      <td>676605.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>526832</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((36.75000 -1.25000, 37.00000 -1.25000...</td>\n","      <td>651071.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>523942</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((34.75000 -0.75000, 35.00000 -0.75000...</td>\n","      <td>443147.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>522501</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((34.75000 -0.50000, 35.00000 -0.50000...</td>\n","      <td>606364.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>518177</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((34.50000 0.25000, 34.75000 0.25000, ...</td>\n","      <td>445679.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   grid_cell_id                                      download_link  \\\n","0        525390  https://biospheric-vector.s3.amazonaws.com/ope...   \n","1        525391  https://biospheric-vector.s3.amazonaws.com/ope...   \n","2        526832  https://biospheric-vector.s3.amazonaws.com/ope...   \n","3        523942  https://biospheric-vector.s3.amazonaws.com/ope...   \n","4        522501  https://biospheric-vector.s3.amazonaws.com/ope...   \n","5        518177  https://biospheric-vector.s3.amazonaws.com/ope...   \n","\n","                                            geometry  buildings_amount  \n","0  POLYGON ((36.50000 -1.00000, 36.75000 -1.00000...          389994.0  \n","1  POLYGON ((36.75000 -1.00000, 37.00000 -1.00000...          676605.0  \n","2  POLYGON ((36.75000 -1.25000, 37.00000 -1.25000...          651071.0  \n","3  POLYGON ((34.75000 -0.75000, 35.00000 -0.75000...          443147.0  \n","4  POLYGON ((34.75000 -0.50000, 35.00000 -0.50000...          606364.0  \n","5  POLYGON ((34.50000 0.25000, 34.75000 0.25000, ...          445679.0  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["dlinks_df = dlinks_df[(dlinks_df.buildings_amount > 370_000)]\n","dlinks_df.index = [i for i in range(len(dlinks_df))]\n","dlinks_df"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid_cell_id</th>\n","      <th>download_link</th>\n","      <th>geometry</th>\n","      <th>buildings_amount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>525391</td>\n","      <td>https://biospheric-vector.s3.amazonaws.com/ope...</td>\n","      <td>POLYGON ((36.75000 -1.00000, 37.00000 -1.00000...</td>\n","      <td>676605.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   grid_cell_id                                      download_link  \\\n","1        525391  https://biospheric-vector.s3.amazonaws.com/ope...   \n","\n","                                            geometry  buildings_amount  \n","1  POLYGON ((36.75000 -1.00000, 37.00000 -1.00000...          676605.0  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["\n","dlinks_df = dlinks_df[~dlinks_df.grid_cell_id.isin(processed_ids)]\n","dlinks_df = dlinks_df.sort_values(by='buildings_amount', ascending=True)\n","dlinks_df"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["676605.0"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["dlinks_df.buildings_amount.sum()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["\n","def extract_geojson(obj_key):\n","     try:\n","        obj = s3.Object('biospheric-vector', obj_key)\n","        obj_bytes = obj.get()['Body'].read()\n","\n","        json_filename = ''\n","        with io.BytesIO(obj_bytes) as tf:\n","        # rewind the file\n","            tf.seek(0)\n","               # Read the file as a zipfile and process the members\n","            with zipfile.ZipFile(tf, mode='r') as zipf:\n","                for subfile in zipf.namelist():\n","                        if '.geojson' in subfile:\n","                            json_filename = subfile\n","                            zipf.extract(subfile, path=geojsons_path)\n","\n","        return os.path.join(geojsons_path, json_filename)\n","     except Exception as e:\n","        print(obj_key, e)\n","        \n","        \n","def read_geojson(geojson_path):\n","    \n","    df = gpd.read_file(geojson_path)\n","    \n","    df['longitude_2'] = df['geometry'].apply(lambda g: g.centroid.xy[0][0])\n","    df['latitude_2'] = df['geometry'].apply(lambda g: g.centroid.xy[1][0])\n","    \n","    df.geometry = shapely.set_precision(df.geometry.array, grid_size=0.000001)\n","    \n","    return df"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# generate coordinate grid with defined tilesize and overlap\n","def generate_grid(\n","                    country_bbox: list,\n","                    tile_bbox: list,\n","                    overlap=0.000\n","                ):\n","    \n","    row_col_dim = tile_bbox\n","    \n","    rows_cols = [\n","      int(abs(country_bbox[0][0] - country_bbox[0][1]) // row_col_dim[0]) if abs(country_bbox[0][0] - country_bbox[0][1]) % row_col_dim[0] == 0 else int(abs(country_bbox[0][0] - country_bbox[0][1]) // row_col_dim[0]) + 1,\n","      int(abs(country_bbox[1][0] - country_bbox[1][1]) // row_col_dim[1]) if abs(country_bbox[1][0] - country_bbox[1][1]) % row_col_dim[1] == 0 else int(abs(country_bbox[1][0] - country_bbox[1][1]) // row_col_dim[1]) + 1\n","    ]\n","    \n","    columns_amount = rows_cols[0]\n","    rows_amount = rows_cols[1]\n","    \n","    tile_width = row_col_dim[0]\n","    tile_height = row_col_dim[1]\n","\n","    tiff_height = abs(country_bbox[1][0] - country_bbox[1][1])\n","    tiff_width = abs(country_bbox[0][0] - country_bbox[0][1])\n","    \n","    images_coords = []\n","    \n","    for col_idx in range(1, columns_amount + 1):\n","    \n","        row_start = country_bbox[0][0] + max(tile_width * (col_idx - 1) - overlap, 0)\n","\n","        if col_idx != columns_amount:\n","\n","            row_limits = [row_start, country_bbox[0][0] + (tile_width * col_idx)]\n","        elif col_idx == columns_amount:\n","            row_limits = [row_start, country_bbox[0][0] + tiff_width]\n","\n","        for row_idx in range(1, rows_amount + 1):\n","\n","            col_start = country_bbox[1][0] + max(tile_height * (row_idx - 1) - overlap, 0)\n","\n","            if row_idx != rows_amount:\n","                col_limits = [col_start, country_bbox[1][0] + (tile_height * row_idx)]\n","            elif row_idx == rows_amount:\n","                col_limits = [col_start, country_bbox[1][0] + tiff_height]\n","\n","            coords = [row_limits, col_limits]\n","            \n","            images_coords.append(coords)\n","\n","    return images_coords\n","\n","\n","# calculate ratio width height ratio\n","def get_bbox_wh_ratio(geometry):\n","    w1, h1, w2, h2 = geometry.bounds\n","    w1 = round(w2 - w1, 8)\n","    h1 = round(h2 - h1, 8)\n","    return w1/h1\n","\n","# calculate distance between buildings\n","def get_distance_between_buildings(building1, building2):\n","    w = building1[0][0] - building2[0][0]\n","    h = building1[1][0] - building2[1][0]\n","\n","    lat_lon_koef = (0.0008988810840833139 + 0.0009006895240588619)/200\n","\n","    distance = np.sqrt((w**2 + h**2)) / lat_lon_koef\n","    if distance < 1:\n","        return 1\n","    \n","    return distance\n","\n","\n","# calculate matching ratios and combine them\n","def calculate_ratios(vida_matched, building, items_treshold):\n","\n","    vida_matched['area_similarity'] = vida_matched['area_in_meters'].apply(lambda a: abs(1 - a/building.area_in_meters))\n","\n","    w1, h1, w2, h2 = building.geometry.bounds\n","    w1 = round(w2 - w1, 8)\n","    h1 = round(h2 - h1, 8)\n","    bwhr = w1/h1\n","\n","    building1 = building.geometry.centroid.xy\n","\n","    vida_matched['shape_similarity'] = vida_matched['geometry'].apply(lambda g: abs(bwhr - get_bbox_wh_ratio(g)))\n","\n","    vida_matched['distance_similarity'] = vida_matched['geometry'].apply(lambda g: abs(1 - 1/get_distance_between_buildings(building1, g.centroid.xy)))\n","\n","    vida_matched['combined_ratios'] = vida_matched['area_similarity'] + vida_matched['shape_similarity'] + vida_matched['distance_similarity']\n","    \n","    vida_matched = vida_matched.sort_values(by='combined_ratios', ascending=True)\n","\n","    # vida_matched = vida_matched.head(items_treshold)\n","    \n","    vida_matched['base_geometry'] = [building.geometry for _ in range(len(vida_matched))]\n","    \n","    vida_matched = vida_matched.drop(\n","        [\n","            'origin_id', \n","            'elec access (%)', \n","            'cons (kWh/month)', \n","            'std cons (kWh/month)',\n","            'a_alls_elec', \n","            'b_alls_elec', \n","            'a_alls_dem', \n","            'b_alls_dem',\n","            'area_in_meters', \n","            'area_similarity', \n","            'shape_similarity', \n","            'distance_similarity',\n","        ],\n","        axis=1\n","    )\n","\n","    return vida_matched\n","\n","def prepare_buildings(vida_df, osm_df, bbox):\n","    \n","    vida_filtered = vida_df[\n","            (vida_df.longitude_2 >= bbox[0][0]) &\n","            (vida_df.longitude_2 <= bbox[0][1]) &\n","            (vida_df.latitude_2 >= bbox[1][0]) &\n","            (vida_df.latitude_2 <= bbox[1][1])\n","    ].drop(['longitude_2', 'latitude_2'], axis=1).copy()\n","    \n","    \n","    osm_filtered = osm_df[\n","            (osm_df.longitude >= bbox[0][0]) &\n","            (osm_df.longitude <= bbox[0][1]) &\n","            (osm_df.latitude >= bbox[1][0]) &\n","            (osm_df.latitude <= bbox[1][1])\n","    ].copy()\n","\n","    if len(osm_filtered) == 0 or len(vida_filtered) == 0:\n","        \n","        pass\n","\n","    else:\n","        # 0,0009\n","        \n","        matched_buildings = []\n","        items_limit = min(len(osm_filtered), len(vida_filtered))\n","#         print(len(osm_filtered), len(vida_filtered))\n","        for idx, building in enumerate(osm_filtered.itertuples()):\n","            try:\n","\n","                max_intersection_row = calculate_ratios(vida_filtered, building, items_limit)\n","                matched_buildings.append(max_intersection_row)\n","            except Exception as e:\n","                print(f'Ratios calculaiton err: {e}')\n","        \n","        try:\n","            df = pd.concat(matched_buildings)\n","#             print(f\"Buildings matched: {len(df)} of {len(osm_filtered)}\")\n","            return df\n","        \n","        except Exception as e:\n","            print(e)\n","            return None\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# algorithm for one to one building matching\n","def match_buildings(main_df, tile_idx):\n","\n","    # assing indexes ang geometry ids\n","    main_df.index = [i  for i in range(len(main_df))]\n","    main_df['geometry_id'] = main_df['base_geometry'].apply(lambda g: str(g))\n","    main_df['secondary_geometry_id'] = main_df['geometry'].apply(lambda g: str(g))\n","    main_df = main_df.drop(['base_geometry', 'geometry'], axis=1)\n","\n","    # main_df.groupby(['geometry_id']).agg({'base_height': max})\n","    vida_keys = {k: v for v, k in enumerate(list(set(main_df['geometry_id'])))}\n","    main_df['g_key'] = main_df['geometry_id'].apply(lambda k: vida_keys[k])\n","\n","\n","    # cost matrix generation\n","    cost_matrix = []\n","\n","    for poly_id in vida_keys.values():\n","\n","        sub_df = main_df[main_df['g_key'] == poly_id].sort_values(by='combined_ratios')\n","        sub_df = sub_df.drop_duplicates(subset='secondary_geometry_id')\n","        cost_matrix.append(list(sub_df.combined_ratios))\n","\n","    cost_matrix = np.array(cost_matrix)\n","\n","    row_idxs, col_idxs = sci_opt.linear_sum_assignment(cost_matrix, maximize=False) \n","\n","    # matches processing\n","    matches_row_idxs = {}\n","    vida_keys_s = {k:v for v, k in vida_keys.items()}\n","\n","    for row_idx, col_idx in zip(row_idxs, col_idxs):\n","\n","        sub_df = main_df[main_df['g_key'] == row_idx].sort_values(by='combined_ratios')\n","\n","        matched_building = main_df.secondary_geometry_id.iloc[col_idx]\n","        # matches[vida_keys_s[row_idx]] = matched_building\n","        matches_row_idxs[vida_keys_s[row_idx]] = [sub_df.combined_ratios.iloc[col_idx], matched_building]\n","\n","    # save all the matches to the json file for further processing\n","    filename = f'matches_row_idxs_{tile_idx}.json'\n","    file_path = f'{jsons_path}/{filename}'\n","    with open(file_path, \"w\") as outfile: \n","        json.dump(matches_row_idxs, outfile)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/42/bkr77x_n1qv118pt28lzd3380000gn/T/ipykernel_80338/3470458869.py:15: DeprecationWarning: jpype._core.isThreadAttachedToJVM is deprecated, use java.lang.Thread.isAttached instead\n","  if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n"]}],"source":["conn = connect_to_db()\n","cursor = conn.cursor()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["processing_state = {\n","    \"total_unique_matched_count\": 0,\n","    \"total_matched_count\": 0\n","}"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def log_state_to_bucket(processing_state: dict):\n","    '''\n","        Function for updating matching state to the 'notebook-job-status' bucket\n","        Each call of this function uploads a selected json file with updated state to the afore mentioned bucket\n","    '''\n","    \n","    filename = f'matching_buildings_Kenya_status_{batch_limit}.json'\n","    with open(filename, \"w\") as outfile:\n","                json.dump(processing_state, outfile)\n","                \n","    cos_client.upload_file(\n","        Filename=filename,\n","        Bucket=config[\"JOB_STATUS_BUCKET\"],\n","        Key=filename,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total_unique_matched_count = 0\n","total_matched_count = 0\n","\n","total_grid_cells = len(dlinks_df)\n","\n","for grid_cell_idx, row in enumerate(tqdm(dlinks_df.itertuples(), desc='Matching buildings:')):\n","    \n","    print(f'Processing cell {row.grid_cell_id}')\n","    os.makedirs(jsons_path, exist_ok = True)\n","    \n","    obj_key = row.download_link.replace(trm,'')\n","    \n","    json_path = extract_geojson(obj_key)\n","    df_energy_estimations = read_geojson(json_path)\n","    \n","    print(f'Buildings with energy estimation: {len(df_energy_estimations)}')\n","    \n","    min_lon, min_lat, max_lon, max_lat = row.geometry.bounds\n","    df_raw = fetch_builings_in_bbox(cursor, min_lon, max_lon, min_lat, max_lat)\n","    \n","    print(f'DB2 buildings: {len(df_raw)}')\n","    \n","    # Kenya bounding box\n","    country_bbox = [\n","        [min_lon, max_lon],\n","        [min_lat, max_lat]\n","    ]\n","\n","    # inside tile bounding box size in lat lon degrees\n","    \n","    if len(df_energy_estimations) < 700_000:\n","        tile_bbox = [.015, .015]\n","    else:\n","        tile_bbox = [.007, .007]\n","\n","    # generate tiles for country\n","    all_country_tiles = generate_grid(country_bbox, tile_bbox, overlap=0.0025)  \n","    \n","    print('grid generated')\n","    # iterate through generated tiles\n","    for tile_idx, bbox in tqdm(enumerate(all_country_tiles), desc='Calculating buildings in tiles', total=len(all_country_tiles)):\n","        # start from specific tile\n","        if tile_idx >= 0:\n","#             print(f'Processing tile {tile_idx} of {len(all_country_tiles)} {round(100*tile_idx/len(all_country_tiles), 2)}%', end='\\r')\n","            filtered_df = prepare_buildings(df_energy_estimations, df_raw, bbox)\n","            if isinstance(filtered_df, pd.DataFrame):\n","                \n","                match_buildings(filtered_df, tile_idx)\n","                \n","    all_files = os.listdir(jsons_path)\n","    matches_row_idxs_jsons = [i for i in all_files if 'matches_row_idxs' in i]\n","    matches_row_idxs_jsons.sort()\n","\n","    all_matches = {}\n","\n","    for i in matches_row_idxs_jsons:\n","\n","        matched = json.load(open(f'{jsons_path}/{i}'))\n","\n","        for k, v in matched.items():\n","            if k in all_matches.keys():\n","                all_matches[k].append([v])\n","            else:\n","                all_matches[k] = [[v]]\n","                \n","                \n","    shutil.rmtree(jsons_path)\n","                \n","    nonunique_matches = [k for k, v in all_matches.items() if len(v) > 1]\n","    unique_matches = [k for k, v in all_matches.items() if len(v) == 1]\n","#     print(f' nonunique_matches {len(nonunique_matches)} unique_matches {len(unique_matches)}')\n","\n","    filtered_matches = {}\n","    for idx, k in enumerate(nonunique_matches):\n","\n","        ratios = []\n","\n","        for i in all_matches[k]:\n","            try:\n","                ratios.append(i[0][0])\n","            except Exception as e:\n","                print(e)\n","\n","        filtered_matches[k] = all_matches[k][ratios.index(min(ratios))][0][1]\n","        \n","\n","    df_raw['geometry_id'] = df_raw['geometry'].apply(lambda g: str(g))\n","    \n","    mapping_dict = {k: all_matches[k][0][0][1] for k in unique_matches}\n","    print(\"Unique matches amount\", len(mapping_dict))\n","\n","    total_unique_matched_count += len(mapping_dict)\n","    # concat unique matches + filtered matches\n","    mapping_dict = mapping_dict | filtered_matches\n","    print(\"All matches amount\", len(mapping_dict))\n","\n","    total_matched_count += len(mapping_dict)\n","    # filter main df only for matched items\n","    \n","    main_df = df_raw[df_raw.geometry_id.isin([i for i in mapping_dict.keys()])].copy()\n","    main_df['secondary_geometry_id'] = main_df['geometry_id'].apply(lambda building_id: mapping_dict[building_id])\n","    \n","    df_energy_estimations['secondary_geometry_id'] = df_energy_estimations['geometry'].astype(str)\n","\n","    df_energy_estimations = df_energy_estimations[[\n","                        'elec access (%)', \n","                        'cons (kWh/month)', \n","                        'std cons (kWh/month)', \n","                        'a_alls_elec', \n","                        'b_alls_elec', \n","                        'a_alls_dem', \n","                        'b_alls_dem',\n","                        'secondary_geometry_id'\n","                    ]].copy()\n","    \n","    main_df = pd.merge(main_df, df_energy_estimations, on='secondary_geometry_id')\n","    \n","    main_df = main_df[['latitude', 'longitude', 'vida_confidence', 'geometry', 'area_in_meters', 'elec access (%)', 'cons (kWh/month)', 'std cons (kWh/month)', 'a_alls_elec', 'b_alls_elec', 'a_alls_dem', 'b_alls_dem']]\n","    \n","    parquet_filename = f'{row.grid_cell_id}_matched_buildings.parquet'\n","    \n","    main_df.drop_duplicates().to_parquet(parquet_filename)\n","    \n","    cos_client.upload_file(Filename=parquet_filename,Bucket=config[\"ENERGY_ESTIMATION_BUCKET\"],Key=parquet_filename)\n","    \n","    processing_state['processed_counts'] = f'Processed: {grid_cell_idx} of {total_grid_cells} | {round(100*grid_cell_idx/total_grid_cells, 3)}%'\n","    processing_state['processed_grid_cell_id'] = row.grid_cell_id\n","    processing_state['total_unique_matched_count'] = total_unique_matched_count\n","    processing_state['total_matched_count'] = total_matched_count\n","    \n","    log_state_to_bucket(processing_state)\n","    \n","    print()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
